version: '3.8'

services:
  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./start-ollama.sh:/start-ollama.sh
    healthcheck:
      test: curl -f http://localhost:11434 || exit 1
      interval: 20s
      retries: 10
    entrypoint: ["/bin/sh", "-c", "/start-ollama.sh"]
    #entrypoint: ["/bin/sh", "-c", "ollama pull llama3 && ollama serve"]
    #entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 2 && ollama pull llama3 && wait"]

  echoworm:
    build: .
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      LLM_BACKEND: ollama
      OLLAMA_BASE_URL: http://ollama:11434
    stdin_open: true
    tty: true

volumes:
  ollama-data:
